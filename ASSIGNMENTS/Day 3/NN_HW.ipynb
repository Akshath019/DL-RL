{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Networks Assignment\n",
    "\n",
    "## Overview\n",
    "This assignment implements neural network components as follows:\n",
    "- **Part 1**: Logic gates (AND, OR, NOR, NAND, XOR) using neurons with sigmoid activation.\n",
    "- **Part 2**: Perceptron for AND and OR gates.\n",
    "- **Part 3**: Feedforward neural network with sigmoid and softmax activations for single and batch inputs.\n",
    "- **Part 4**: Debugging incorrect logic gate parameters to demonstrate understanding of errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Sigmoid function\n",
    "def sigmoid(x):\n",
    "    \"\"\"Sigmoid function: 1 / (1 + e^-x)\"\"\"\n",
    "    return 1.0 / (1.0 + np.exp(-x))\n",
    "\n",
    "# Plot sigmoid function\n",
    "vals = np.linspace(-10, 10, num=100, dtype=np.float32)\n",
    "activation = sigmoid(vals)\n",
    "fig = plt.figure(figsize=(12,6))\n",
    "plt.plot(vals, activation)\n",
    "plt.grid(True, which='both')\n",
    "plt.axhline(y=0, color='k')\n",
    "plt.axvline(x=0, color='k')\n",
    "plt.yticks()\n",
    "plt.ylim([-0.5, 1.5])\n",
    "plt.title('Sigmoid Activation Function')\n",
    "plt.xlabel('Input')\n",
    "plt.ylabel('Output')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Neurons as Logic Gates\n",
    "\n",
    "We implement logic gates using a single neuron with sigmoid activation, where $z = w_1 x_1 + w_2 x_2 + b$ and output is $\\sigma(z)$. Inputs $x_1, x_2 \\in \\{0, 1\\}$, and weights/biases are chosen to produce outputs close to 0 or 1 per the truth tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logic gate helper function\n",
    "def logic_gate(w1, w2, b):\n",
    "    \"\"\"Create a logic gate with weights w1, w2 and bias b\"\"\"\n",
    "    return lambda x1, x2: sigmoid(w1 * x1 + w2 * x2 + b)\n",
    "\n",
    "# Test function for logic gates\n",
    "def test(gate, gate_name):\n",
    "    \"\"\"Test logic gate and print truth table\"\"\"\n",
    "    print(f\"{gate_name} Gate:\")\n",
    "    for a, b in [(0, 0), (0, 1), (1, 0), (1, 1)]:\n",
    "        print(f\"{a}, {b}: {np.round(gate(a, b))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OR Gate\n",
    "\n",
    "<table>\n",
    "<tr><th colspan=\"3\">OR gate truth table</th></tr>\n",
    "<tr><th colspan=\"2\">Input</th><th>Output</th></tr>\n",
    "<tr><td>0</td><td>0</td><td>0</td></tr>\n",
    "<tr><td>0</td><td>1</td><td>1</td></tr>\n",
    "<tr><td>1</td><td>0</td><td>1</td></tr>\n",
    "<tr><td>1</td><td>1</td><td>1</td></tr>\n",
    "</table>\n",
    "\n",
    "Parameters: $w_1=20, w_2=20, b=-10$ ensure $z$ is negative for (0,0) and positive otherwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "or_gate = logic_gate(20, 20, -10)\n",
    "test(or_gate, \"OR\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AND Gate\n",
    "\n",
    "<table>\n",
    "<tr><th colspan=\"3\">AND gate truth table</th></tr>\n",
    "<tr><th colspan=\"2\">Input</th><th>Output</th></tr>\n",
    "<tr><td>0</td><td>0</td><td>0</td></tr>\n",
    "<tr><td>0</td><td>1</td><td>0</td></tr>\n",
    "<tr><td>1</td><td>0</td><td>0</td></tr>\n",
    "<tr><td>1</td><td>1</td><td>1</td></tr>\n",
    "</table>\n",
    "\n",
    "Parameters: $w_1=15, w_2=15, b=-20$ ensure $z$ is positive only for (1,1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "and_gate = logic_gate(15, 15, -20)\n",
    "test(and_gate, \"AND\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NOR Gate\n",
    "\n",
    "<table>\n",
    "<tr><th colspan=\"3\">NOR gate truth table</th></tr>\n",
    "<tr><th colspan=\"2\">Input</th><th>Output</th></tr>\n",
    "<tr><td>0</td><td>0</td><td>1</td></tr>\n",
    "<tr><td>0</td><td>1</td><td>0</td></tr>\n",
    "<tr><td>1</td><td>0</td><td>0</td></tr>\n",
    "<tr><td>1</td><td>1</td><td>0</td></tr>\n",
    "</table>\n",
    "\n",
    "Parameters: $w_1=-20, w_2=-20, b=10$ ensure $z$ is positive for (0,0) and negative otherwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nor_gate = logic_gate(-20, -20, 10)\n",
    "test(nor_gate, \"NOR\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NAND Gate\n",
    "\n",
    "<table>\n",
    "<tr><th colspan=\"3\">NAND gate truth table</th></tr>\n",
    "<tr><th colspan=\"2\">Input</th><th>Output</th></tr>\n",
    "<tr><td>0</td><td>0</td><td>1</td></tr>\n",
    "<tr><td>0</td><td>1</td><td>1</td></tr>\n",
    "<tr><td>1</td><td>0</td><td>1</td></tr>\n",
    "<tr><td>1</td><td>1</td><td>0</td></tr>\n",
    "</table>\n",
    "\n",
    "Parameters: $w_1=-15, w_2=-15, b=20$ ensure $z$ is negative only for (1,1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nand_gate = logic_gate(-15, -15, 20)\n",
    "test(nand_gate, \"NAND\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XOR Gate\n",
    "\n",
    "<table>\n",
    "<tr><th colspan=\"3\">XOR gate truth table</th></tr>\n",
    "<tr><th colspan=\"2\">Input</th><th>Output</th></tr>\n",
    "<tr><td>0</td><td>0</td><td>0</td></tr>\n",
    "<tr><td>0</td><td>1</td><td>1</td></tr>\n",
    "<tr><td>1</td><td>0</td><td>1</td></tr>\n",
    "<tr><td>1</td><td>1</td><td>0</td></tr>\n",
    "</table>\n",
    "\n",
    "XOR is implemented as $AND(OR(x_1, x_2), NAND(x_1, x_2))$, as a single neuron cannot model XOR."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def xor_gate(a, b):\n",
    "    \"\"\"XOR gate using OR, NAND, and AND gates\"\"\"\n",
    "    c = or_gate(a, b)\n",
    "    d = nand_gate(a, b)\n",
    "    return and_gate(c, d)\n",
    "\n",
    "test(xor_gate, \"XOR\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Perceptron for AND and OR Gates\n",
    "\n",
    "We implement a perceptron with a step function activation to model AND and OR gates, which are linearly separable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perceptron class\n",
    "class Perceptron:\n",
    "    def __init__(self, weights, bias):\n",
    "        self.weights = np.array(weights)\n",
    "        self.bias = bias\n",
    "\n",
    "    def step_function(self, x):\n",
    "        return 1 if x >= 0 else 0\n",
    "\n",
    "    def predict(self, inputs):\n",
    "        weighted_sum = np.dot(inputs, self.weights) + self.bias\n",
    "        return self.step_function(weighted_sum)\n",
    "\n",
    "# Test perceptron\n",
    "def test_perceptron(perceptron, gate_name):\n",
    "    print(f\"{gate_name} Perceptron:\")\n",
    "    inputs = [(0, 0), (0, 1), (1, 0), (1, 1)]\n",
    "    for x1, x2 in inputs:\n",
    "        print(f\"{x1}, {x2}: {perceptron.predict([x1, x2])}\")\n",
    "\n",
    "# AND Perceptron: w1=1, w2=1, b=-1.5\n",
    "and_perceptron = Perceptron([1, 1], -1.5)\n",
    "test_perceptron(and_perceptron, \"AND\")\n",
    "\n",
    "# OR Perceptron: w1=1, w2=1, b=-0.5\n",
    "or_perceptron = Perceptron([1, 1], -0.5)\n",
    "test_perceptron(or_perceptron, \"OR\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Feedforward Neural Network\n",
    "\n",
    "We compute the forward pass of a three-layer neural network with sigmoid activations for hidden layers and softmax for the output layer. We provide functions for single and batch inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weight matrices\n",
    "W_1 = np.array([[2, -1, 1, 4], [-1, 2, -3, 1], [3, -2, -1, 5]])\n",
    "W_2 = np.array([[3, 1, -2, 1], [-2, 4, 1, -4], [-1, -3, 2, -5], [3, 1, 1, 1]])\n",
    "W_3 = np.array([[-1, 3, -2], [1, -1, -3], [3, -2, 2], [1, 2, 1]])\n",
    "\n",
    "# Input data\n",
    "x_in = np.array([0.5, 0.8, 0.2])\n",
    "x_mat_in = np.array([\n",
    "    [0.5, 0.8, 0.2],\n",
    "    [0.1, 0.9, 0.6],\n",
    "    [0.2, 0.2, 0.3],\n",
    "    [0.6, 0.1, 0.9],\n",
    "    [0.5, 0.5, 0.4],\n",
    "    [0.9, 0.1, 0.9],\n",
    "    [0.1, 0.8, 0.7]\n",
    "])\n",
    "\n",
    "# Softmax functions\n",
    "def soft_max_vec(vec):\n",
    "    return np.exp(vec) / np.sum(np.exp(vec))\n",
    "\n",
    "def soft_max_mat(mat):\n",
    "    return np.exp(mat) / np.sum(np.exp(mat), axis=1).reshape(-1, 1)\n",
    "\n",
    "# Compute layer inputs and outputs for x_in\n",
    "z1 = x_in @ W_1\n",
    "a1 = sigmoid(z1)\n",
    "z2 = a1 @ W_2\n",
    "a2 = sigmoid(z2)\n",
    "z3 = a2 @ W_3\n",
    "output = soft_max_vec(z3)\n",
    "\n",
    "print(\"Layer 1 input (z1):\", z1)\n",
    "print(\"Layer 1 output (a1):\", a1)\n",
    "print(\"Layer 2 input (z2):\", z2)\n",
    "print(\"Layer 2 output (a2):\", a2)\n",
    "print(\"Layer 3 input (z3):\", z3)\n",
    "print(\"Network output (softmax):\", output)\n",
    "\n",
    "# Functions for neural network forward pass\n",
    "def nn_forward_single(x):\n",
    "    \"\"\"Forward pass for single input\"\"\"\n",
    "    a1 = sigmoid(x @ W_1)\n",
    "    a2 = sigmoid(a1 @ W_2)\n",
    "    out = soft_max_vec(a2 @ W_3)\n",
    "    return out\n",
    "\n",
    "def nn_forward_batch(X):\n",
    "    \"\"\"Forward pass for batch inputs\"\"\"\n",
    "    a1 = sigmoid(X @ W_1)\n",
    "    a2 = sigmoid(a1 @ W_2)\n",
    "    out = soft_max_mat(a2 @ W_3)\n",
    "    return out\n",
    "\n",
    "# Test functions\n",
    "print(\"\\nSingle input output:\", nn_forward_single(x_in))\n",
    "print(\"Batch input output:\\n\", nn_forward_batch(x_mat_in))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Debugging Incorrect Logic Gate Parameters\n",
    "\n",
    "We analyze the impact of incorrect NOR and NAND parameters (`w1=0, w2=0, b=0`) observed in some submissions, which caused XOR to fail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test incorrect NOR and NAND gates\n",
    "incorrect_nor_gate = logic_gate(0, 0, 0)\n",
    "incorrect_nand_gate = logic_gate(0, 0, 0)\n",
    "\n",
    "print(\"Incorrect NOR Gate (w1=0, w2=0, b=0):\")\n",
    "test(incorrect_nor_gate, \"NOR\")\n",
    "\n",
    "print(\"\\nIncorrect NAND Gate (w1=0, w2=0, b=0):\")\n",
    "test(incorrect_nand_gate, \"NAND\")\n",
    "\n",
    "# Test XOR with incorrect NOR\n",
    "def incorrect_xor_gate_nor(a, b):\n",
    "    c = or_gate(a, b)\n",
    "    d = incorrect_nor_gate(a, b)\n",
    "    return and_gate(c, d)\n",
    "\n",
    "print(\"\\nXOR with Incorrect NOR:\")\n",
    "test(incorrect_xor_gate_nor, \"XOR\")\n",
    "\n",
    "# Test XOR with incorrect NAND\n",
    "def incorrect_xor_gate_nand(a, b):\n",
    "    c = or_gate(a, b)\n",
    "    d = incorrect_nand_gate(a, b)\n",
    "    return and_gate(c, d)\n",
    "\n",
    "print(\"\\nXOR with Incorrect NAND:\")\n",
    "test(incorrect_xor_gate_nand, \"XOR\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Debugging Analysis\n",
    "\n",
    "- **Incorrect Parameters** (`w1=0, w2=0, b=0`):\n",
    "  - For all inputs, $z = 0$, so $\\sigma(z) = 0.5$, rounded to `0.0` in tests.\n",
    "  - NOR should output `(0,0:1), (0,1:0), (1,0:0), (1,1:0)`; instead, all outputs are `0`.\n",
    "  - NAND should output `(0,0:1), (0,1:1), (1,0:1), (1,1:0)`; instead, all outputs are `0`.\n",
    "\n",
    "- **Impact on XOR**:\n",
    "  - XOR = $AND(OR(a,b), NAND(a,b))$.\n",
    "  - With incorrect NAND, `NAND(a,b)=0` for all inputs, so `AND(c,0)=0`, causing XOR to output `0` for all inputs.\n",
    "  - With incorrect NOR, similar issues arise, as `NOR(a,b)=0` disrupts the AND gate logic.\n",
    "  - This explains the incorrect XOR output `(1,1:1)` in some submissions, as the intermediate gates fail.\n",
    "\n",
    "- **Correct Parameters**:\n",
    "  - NOR: `w1=-20, w2=-20, b=10` produces $z=10$ for (0,0) and negative otherwise.\n",
    "  - NAND: `w1=-15, w2=-15, b=20` produces $z=20$ for non-(1,1) inputs and $z=-10$ for (1,1).\n",
    "  - These ensure correct XOR: `(0,0:0), (0,1:1), (1,0:1), (1,1:0)`."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}